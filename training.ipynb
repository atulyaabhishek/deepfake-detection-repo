{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2deda22-b5a1-4483-978a-dab887cc302e",
   "metadata": {},
   "source": [
    "# Building a Deepfake Detector using Deep Learning Models\n",
    "This notebook demonstrates the development of a deepfake detection system using multiple pre-trained **CNN (Convolutional Neural Network)** models, such as **ResNet50**, **EfficientNetV2B0** and **Xception**, combined with **LSTM (Long Short-Term Memory)** networks for temporal analysis. The datasets used are **FaceForensics++**, **DFDC** and **Celeb-DF (v2)**. To ensure unbiased testing, the **Celeb-DF (v2)** dataset consists of completely unseen videos that are exclusively reserved for testing and are not included in the training or validation processes. `OpenCV` is utilized for video frame extraction and preprocessing while `dlib` is used for face detection and cropping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c6954-0e0f-4066-aa49-0e18cdd2200e",
   "metadata": {},
   "source": [
    "## GPU Configuration and Verification with TensorFlow\n",
    "To ensure TensorFlow is configured to effectively utilize the GPU for deep learning tasks, optimize memory usage and verify GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a36f677-7ff5-4a5e-acce-bf6532ad3ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow CUDA Support: True\n",
      "Num GPUs Available: 1\n",
      "Enabled memory growth for GPU 0: NVIDIA GeForce 940MX\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow is built with CUDA support and list GPUs\n",
    "print(\"TensorFlow CUDA Support:\", tf.test.is_built_with_cuda())\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(physical_devices))\n",
    "\n",
    "if physical_devices:\n",
    "    try:\n",
    "        for i, gpu in enumerate(physical_devices):\n",
    "            # Enable memory growth for each GPU\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Enabled memory growth for GPU {i}: {tf.config.experimental.get_device_details(gpu)['device_name']}\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error enabling GPU memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected. Ensure proper GPU setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d1612-f084-4f65-9623-408bb9ea0fae",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "Importing all necessary libraries at the top to ensure better organization, easy debugging and smooth execution of the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11407ff-6aa0-4e6a-bd98-d4f017401ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Dropout, LSTM, TimeDistributed, Concatenate\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetV2B0, Xception\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, Callback, ReduceLROnPlateau\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516d32d-0fbe-42b4-87da-0d4c19514c95",
   "metadata": {},
   "source": [
    "## 3. Utility Functions and Custom Callbacks for Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3251df5b-679e-4635-95ef-f24f7f952165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Logger\n",
    "class LearningRateLogger(Callback):\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr.numpy()\n",
    "        self.logs.append({'epoch': epoch + 1, 'learning_rate': lr})\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        df.to_excel(self.writer, sheet_name='Learning Rates', index=False)\n",
    "\n",
    "# Precision, Recall, F1 Score Logger\n",
    "class MetricsLogger(Callback):\n",
    "    def __init__(self, validation_data, writer):\n",
    "        self.validation_data = validation_data\n",
    "        self.writer = writer\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Extract a single batch from the generator\n",
    "        val_x, val_y_true = next(self.validation_data)\n",
    "        val_y_pred = (self.model.predict(val_x) > 0.5).astype(\"int32\").flatten()\n",
    "        val_y_true = val_y_true.flatten()\n",
    "\n",
    "        # Compute metrics with zero_division=0 for precision\n",
    "        precision = precision_score(val_y_true, val_y_pred, zero_division=0)\n",
    "        recall = recall_score(val_y_true, val_y_pred)\n",
    "        f1 = f1_score(val_y_true, val_y_pred)\n",
    "\n",
    "        # Append metrics to logs\n",
    "        self.logs.append({'epoch': epoch + 1, 'precision': precision, 'recall': recall, 'f1_score': f1})\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        df.to_excel(self.writer, sheet_name='Precision-Recall-F1', index=False)\n",
    "\n",
    "# Epoch Time Logger\n",
    "class EpochTimeLogger(Callback):\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        duration = time.time() - self.start_time\n",
    "        self.logs.append({'epoch': epoch + 1, 'duration': duration})\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        df.to_excel(self.writer, sheet_name='Epoch Times', index=False)\n",
    "\n",
    "# Batch Metrics Logger\n",
    "class BatchMetricsLogger(Callback):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        with open(self.filename, 'w') as f:\n",
    "            f.write('epoch,batch,loss,accuracy,val_loss,val_accuracy\\n')\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        epoch = self.params['epochs']\n",
    "        with open(self.filename, 'a') as f:\n",
    "            f.write(f'{epoch},{batch},{logs.get(\"loss\", 0):.4f},{logs.get(\"accuracy\", 0):.4f},,,\\n')\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        epoch = self.params['epochs']\n",
    "        with open(self.filename, 'a') as f:\n",
    "            f.write(\n",
    "                f'{epoch},{batch},,,{logs.get(\"val_loss\", 0):.4f},{logs.get(\"val_accuracy\", 0):.4f}\\n')\n",
    "\n",
    "# Custom Callbacks\n",
    "def create_callbacks(model_name, val_generator):\n",
    "    log_writer = pd.ExcelWriter(f'{model_name}_training_logs.xlsx', engine='openpyxl')\n",
    "    return [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ModelCheckpoint(f'{model_name}_model.h5', save_best_only=True, monitor='val_loss', verbose=1),\n",
    "        CSVLogger(f'{model_name}_training_log.csv'),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "        LearningRateLogger(writer=log_writer),\n",
    "        MetricsLogger(validation_data=val_generator, writer=log_writer),\n",
    "        EpochTimeLogger(writer=log_writer),\n",
    "        BatchMetricsLogger(filename=f'{model_name}_batch_logs.csv')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20b403-2f67-43b5-a4eb-86a411b5dea1",
   "metadata": {},
   "source": [
    "## 4. Augmentation and Sequence Generation\n",
    "Enhancing model generalization through data augmentation and implementing a sequence generator to efficiently feed sequential image frames into CNN-LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45356454-9a17-48e5-9942-816bcd906e47",
   "metadata": {},
   "source": [
    "### 4.1 Defining Custom Sequence Generator for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b467cb7-e85e-41ac-a6e0-3b9b046ef51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sequence_data_generator(\n",
    "    data_dir, batch_size, sequence_length, target_size=(224, 224), augment=True, max_iterations=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Stratified data generator for training and validation.\n",
    "\n",
    "    Preprocessing (resizing, normalization) is applied first, followed by augmentation (if enabled).\n",
    "    Stratified sampling ensures balanced class representation in each batch.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the dataset directory containing class subfolders ('real', 'fake').\n",
    "        batch_size (int): Number of sequences per batch.\n",
    "        sequence_length (int): Number of frames per sequence.\n",
    "        target_size (tuple): Dimensions to resize each frame to (height, width).\n",
    "        augment (bool): Whether to apply augmentation to images.\n",
    "        max_iterations (int): Maximum number of iterations for batch generation (useful for debugging).\n",
    "        shuffle (bool): Whether to shuffle sequences within and across classes.\n",
    "\n",
    "    Yields:\n",
    "        Tuple of (X, y): X is a batch of sequences, y is the corresponding labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Initializing Stratified Sequence Data Generator ---\")\n",
    "    print(f\"Data Directory: {data_dir}\")\n",
    "    print(f\"Batch Size: {batch_size}, Sequence Length: {sequence_length}, Target Size: {target_size}, Augment: {augment}, Shuffle: {shuffle}\\n\")\n",
    "\n",
    "    # Define augmentation pipeline\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10 if augment else 0,\n",
    "        zoom_range=0.1 if augment else 0,\n",
    "        horizontal_flip=augment,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    # Class directories and labels\n",
    "    class_dirs = {\"real\": 0, \"fake\": 1}\n",
    "    file_paths = {class_name: [] for class_name in class_dirs.keys()}\n",
    "\n",
    "    # Collect image paths for each class\n",
    "    print(\"Collecting image paths and labels...\")\n",
    "    for class_name, label in class_dirs.items():\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "\n",
    "        # Validate class directory\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Error: Class directory {class_dir} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Scanning directory: {class_dir}\")\n",
    "        \n",
    "        # Recursively collect image paths from subfolders\n",
    "        for subdir, _, files in os.walk(class_dir):  # Traverse subdirectories\n",
    "            for file_name in files:\n",
    "                if file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    file_paths[class_name].append(os.path.join(subdir, file_name))\n",
    "\n",
    "        print(f\"Collected {len(file_paths[class_name])} images for class '{class_name}'.\")\n",
    "\n",
    "        if len(file_paths[class_name]) == 0:\n",
    "            print(f\"Warning: No images found for class '{class_name}'. This class will be skipped.\")\n",
    "\n",
    "        # Handle class imbalance by oversampling\n",
    "        max_class_size = max(len(file_paths[\"real\"]), len(file_paths[\"fake\"]))\n",
    "        if len(file_paths[class_name]) < max_class_size:\n",
    "            oversampling_factor = (max_class_size // len(file_paths[class_name])) + 1\n",
    "            file_paths[class_name] *= oversampling_factor\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(file_paths[class_name])  # Initial shuffling within the class\n",
    "\n",
    "    print(\"\\nImage collection and shuffling completed for all classes.\")\n",
    "\n",
    "    # Ensure class balance per batch\n",
    "    class_batch_size = batch_size // len(class_dirs)\n",
    "    assert class_batch_size > 0, \"Batch size must be greater than the number of classes.\"\n",
    "\n",
    "    # Batch generation loop\n",
    "    iterations = 0\n",
    "    while max_iterations is None or iterations < max_iterations:\n",
    "        X, y = [], []\n",
    "\n",
    "        for class_name, label in class_dirs.items():\n",
    "            for _ in range(class_batch_size):\n",
    "                # Randomly select a starting index for the sequence\n",
    "                if len(file_paths[class_name]) < sequence_length:\n",
    "                    print(f\"Error: Not enough images in class '{class_name}' to form sequences.\")\n",
    "                    continue\n",
    "\n",
    "                start_idx = np.random.randint(0, len(file_paths[class_name]) - sequence_length + 1)\n",
    "\n",
    "                sequence = []\n",
    "                augmentation_params = None  # Ensure consistent augmentations across frames in a sequence\n",
    "\n",
    "                for frame_index in range(sequence_length):\n",
    "                    img_path = file_paths[class_name][start_idx + frame_index]\n",
    "                    img = cv2.imread(img_path)\n",
    "\n",
    "                    # Validate file loading\n",
    "                    if img is None:\n",
    "                        print(f\"Warning: Failed to load image: {img_path}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Preprocessing: Resize, normalize\n",
    "                    try:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                        img = cv2.resize(img, target_size)\n",
    "                        img = img / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "                        # Apply consistent augmentations across the sequence\n",
    "                        if augment:\n",
    "                            if augmentation_params is None:\n",
    "                                augmentation_params = datagen.get_random_transform(img.shape)\n",
    "                            img = datagen.apply_transform(img, augmentation_params)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing image {img_path}: {e}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    sequence.append(img)\n",
    "\n",
    "                X.append(sequence)\n",
    "                y.append(label)\n",
    "\n",
    "        # Final shuffle to mix sequences from different classes\n",
    "        if shuffle:\n",
    "            combined = list(zip(X, y))\n",
    "            np.random.shuffle(combined)\n",
    "            X, y = zip(*combined)\n",
    "\n",
    "        yield np.array(X), np.array(y)\n",
    "        iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09040a1a-c737-45d5-b8b2-f83093f29c3f",
   "metadata": {},
   "source": [
    "### 4.2 Initializing Data Generators for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437eb964-4615-487f-a0d7-21f1314cdece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Up Training and Validation Dataset Paths ---\n",
      "Training Directory: C:\\Users\\atul\\Cropped_Faces\\train\n",
      "Validation Directory: C:\\Users\\atul\\Cropped_Faces\\val\n",
      "\n",
      "--- Creating Stratified Training Generator ---\n",
      "Training generator created successfully.\n",
      "\n",
      "--- Creating Stratified Validation Generator ---\n",
      "Validation generator created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset directories\n",
    "print(\"--- Setting Up Training and Validation Dataset Paths ---\")\n",
    "base_dir = os.getcwd()\n",
    "train_dir = os.path.join(base_dir, \"Cropped_Faces\", \"train\")\n",
    "val_dir = os.path.join(base_dir, \"Cropped_Faces\", \"val\")\n",
    "print(f\"Training Directory: {train_dir}\")\n",
    "print(f\"Validation Directory: {val_dir}\")\n",
    "\n",
    "# Hyperparameters for Testing/Validation\n",
    "BATCH_SIZE = 8  # Recommended batch size is 8 (Reduce value according to need)\n",
    "SEQUENCE_LENGTH = 10  # Recommended frames per sequence is 10 (Reduce value according to need)\n",
    "TARGET_SIZE = (224, 224)  # Resize dimensions\n",
    "\n",
    "# Initialize training generator (with stratified sampling and augmentation)\n",
    "print(\"\\n--- Creating Stratified Training Generator ---\")\n",
    "train_generator = stratified_sequence_data_generator(\n",
    "    data_dir=train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    augment=True  # Enable augmentation\n",
    ")\n",
    "print(\"Training generator created successfully.\")\n",
    "\n",
    "# Initialize validation generator (no augmentation, stratified sampling)\n",
    "print(\"\\n--- Creating Stratified Validation Generator ---\")\n",
    "val_generator = stratified_sequence_data_generator(\n",
    "    data_dir=val_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    target_size=TARGET_SIZE,\n",
    "    augment=False  # No augmentation for validation\n",
    ")\n",
    "print(\"Validation generator created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780239d-5713-4922-b520-72b6675c45a7",
   "metadata": {},
   "source": [
    "## 5. Model Definition\n",
    "Defining the CNN-LSTM architectures, including separate models for ResNet50, EfficientNetV2B0 and Xception, along with a combined model that fuses features from all three networks, while incorporating an LSTM layer to capture temporal dependencies in the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27f833a-b54d-48c9-bae1-7ec37f1d5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CNN-LSTM Model\n",
    "def create_cnn_lstm_model(feature_extractor, sequence_length):\n",
    "    print(f\"Creating CNN-LSTM model for {feature_extractor.name}...\")\n",
    "    feature_extractor.trainable = False  # Freeze the pre-trained feature extractor\n",
    "    model = Sequential([\n",
    "        TimeDistributed(feature_extractor, input_shape=(sequence_length, *TARGET_SIZE, 3)),\n",
    "        TimeDistributed(GlobalAveragePooling2D()),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    print(f\"{feature_extractor.name}-based CNN-LSTM model created successfully.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd5005-19de-476d-ac03-0c256d09a03b",
   "metadata": {},
   "source": [
    "### 5.1 Loading ResNet50 Pre-trained Feature Extractor and Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f2f1b0-f971-4397-9a18-404e11c812e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 feature extractor loaded successfully.\n",
      "\n",
      "Creating CNN-LSTM model for resnet50...\n",
      "resnet50-based CNN-LSTM model created successfully.\n",
      "\n",
      "--- Compiling ResNet50 Model ---\n",
      "ResNet50 model compiled successfully.\n",
      "\n",
      "--- ResNet50 Model Summary ---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 10, 7, 7, 2048)   23587712  \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 10, 2048)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               1114624   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,718,977\n",
      "Trainable params: 1,131,265\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet50 feature extractors\n",
    "resnet_extractor = ResNet50(weights='imagenet', include_top=False, input_shape=(*TARGET_SIZE, 3))\n",
    "print(\"ResNet50 feature extractor loaded successfully.\\n\")\n",
    "\n",
    "# Initialize ResNet50 Model\n",
    "resnet_model = create_cnn_lstm_model(resnet_extractor, SEQUENCE_LENGTH)\n",
    "\n",
    "# Compile ResNet50 Model\n",
    "print(\"\\n--- Compiling ResNet50 Model ---\")\n",
    "resnet_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"ResNet50 model compiled successfully.\\n\")\n",
    "\n",
    "# Print ResNet50 Model Summary\n",
    "print(\"--- ResNet50 Model Summary ---\")\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9008b0-8dae-4b8b-8e9c-b22230db2106",
   "metadata": {},
   "source": [
    "### 5.2 Callbacks for ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341de3f7-75cc-4bc2-8ce2-aa3f5d1db2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Up Training Callbacks for ResNet ---\n",
      "EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau and ExcelWriter callbacks initialized for ResNet50 model.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks for ResNet\n",
    "print(\"--- Setting Up Training Callbacks for ResNet ---\")\n",
    "\n",
    "# Standard Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_resnet = ModelCheckpoint('resnet_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "csv_logger_resnet = CSVLogger('resnet_training_log.csv')\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',        # Monitor validation loss\n",
    "    factor=0.5,                # Reduce learning rate by half\n",
    "    patience=3,                # Wait for 3 epochs of no improvement\n",
    "    min_lr=1e-6,               # Minimum learning rate\n",
    "    verbose=1                  # Display logs\n",
    ")\n",
    "\n",
    "# Custom Logging Callbacks\n",
    "log_writer_resnet = pd.ExcelWriter('resnet_training_logs.xlsx', engine='openpyxl')\n",
    "\n",
    "lr_logger_resnet = LearningRateLogger(writer=log_writer_resnet)\n",
    "metrics_logger_resnet = MetricsLogger(validation_data=val_generator, writer=log_writer_resnet)\n",
    "epoch_time_logger_resnet = EpochTimeLogger(writer=log_writer_resnet)\n",
    "batch_metrics_logger_resnet = BatchMetricsLogger(filename='resnet_batch_logs.csv')\n",
    "\n",
    "print(\"EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau and ExcelWriter callbacks initialized for ResNet50 model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe6136-b16e-43a2-896b-f89f633e4471",
   "metadata": {},
   "source": [
    "### 5.3 Training ResNet-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ad2264-5f44-4ade-acea-54af4cf2ac32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training ResNet Model ---\n",
      "\n",
      "--- Initializing Stratified Sequence Data Generator ---\n",
      "Data Directory: C:\\Users\\atul\\Cropped_Faces\\train\n",
      "Batch Size: 8, Sequence Length: 10, Target Size: (224, 224), Augment: True, Shuffle: True\n",
      "\n",
      "Collecting image paths and labels...\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\train\\real\n",
      "Collected 139606 images for class 'real'.\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\train\\fake\n",
      "Collected 123036 images for class 'fake'.\n",
      "\n",
      "Image collection and shuffling completed for all classes.\n",
      "\n",
      "Generating a batch of 8 sequences...\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.4879\n",
      "--- Initializing Stratified Sequence Data Generator ---\n",
      "Data Directory: C:\\Users\\atul\\Cropped_Faces\\val\n",
      "Batch Size: 8, Sequence Length: 10, Target Size: (224, 224), Augment: False, Shuffle: True\n",
      "\n",
      "Collecting image paths and labels...\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\val\\real\n",
      "Collected 34902 images for class 'real'.\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\val\\fake\n",
      "Collected 30760 images for class 'fake'.\n",
      "\n",
      "Image collection and shuffling completed for all classes.\n",
      "\n",
      "Generating a batch of 8 sequences...\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69363, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "300/300 [==============================] - 1265s 4s/step - loss: 0.7063 - accuracy: 0.4879 - val_loss: 0.6936 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6950 - accuracy: 0.5050\n",
      "Epoch 2: val_loss improved from 0.69363 to 0.69305, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "300/300 [==============================] - 1241s 4s/step - loss: 0.6950 - accuracy: 0.5050 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6947 - accuracy: 0.5067\n",
      "Epoch 3: val_loss did not improve from 0.69305\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "300/300 [==============================] - 1246s 4s/step - loss: 0.6947 - accuracy: 0.5067 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.5025\n",
      "Epoch 4: val_loss did not improve from 0.69305\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "300/300 [==============================] - 1242s 4s/step - loss: 0.6938 - accuracy: 0.5025 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.4988\n",
      "Epoch 5: val_loss did not improve from 0.69305\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "300/300 [==============================] - 1239s 4s/step - loss: 0.6946 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6942 - accuracy: 0.4996\n",
      "Epoch 6: val_loss improved from 0.69305 to 0.69304, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "300/300 [==============================] - 1241s 4s/step - loss: 0.6942 - accuracy: 0.4996 - val_loss: 0.6930 - val_accuracy: 0.5142 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.4938\n",
      "Epoch 7: val_loss did not improve from 0.69304\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "300/300 [==============================] - 1241s 4s/step - loss: 0.6949 - accuracy: 0.4938 - val_loss: 0.6933 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.5163\n",
      "Epoch 8: val_loss improved from 0.69304 to 0.69272, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "300/300 [==============================] - 1241s 4s/step - loss: 0.6928 - accuracy: 0.5163 - val_loss: 0.6927 - val_accuracy: 0.5300 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.5088\n",
      "Epoch 9: val_loss did not improve from 0.69272\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "300/300 [==============================] - 1239s 4s/step - loss: 0.6934 - accuracy: 0.5088 - val_loss: 0.6927 - val_accuracy: 0.5217 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5104\n",
      "Epoch 10: val_loss did not improve from 0.69272\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "300/300 [==============================] - 1240s 4s/step - loss: 0.6927 - accuracy: 0.5104 - val_loss: 0.6936 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6935 - accuracy: 0.5096\n",
      "Epoch 11: val_loss improved from 0.69272 to 0.69189, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "300/300 [==============================] - 1240s 4s/step - loss: 0.6935 - accuracy: 0.5096 - val_loss: 0.6919 - val_accuracy: 0.5483 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5188\n",
      "Epoch 12: val_loss improved from 0.69189 to 0.69144, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "300/300 [==============================] - 1240s 4s/step - loss: 0.6918 - accuracy: 0.5188 - val_loss: 0.6914 - val_accuracy: 0.5717 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5150\n",
      "Epoch 13: val_loss improved from 0.69144 to 0.68853, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "300/300 [==============================] - 1242s 4s/step - loss: 0.6918 - accuracy: 0.5150 - val_loss: 0.6885 - val_accuracy: 0.5458 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5100\n",
      "Epoch 14: val_loss did not improve from 0.68853\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "300/300 [==============================] - 1239s 4s/step - loss: 0.6924 - accuracy: 0.5100 - val_loss: 0.6918 - val_accuracy: 0.5208 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6940 - accuracy: 0.5071\n",
      "Epoch 15: val_loss did not improve from 0.68853\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "300/300 [==============================] - 1241s 4s/step - loss: 0.6940 - accuracy: 0.5071 - val_loss: 0.6913 - val_accuracy: 0.5417 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.5179\n",
      "Epoch 16: val_loss did not improve from 0.68853\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "300/300 [==============================] - 1239s 4s/step - loss: 0.6924 - accuracy: 0.5179 - val_loss: 0.6904 - val_accuracy: 0.5667 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.5238\n",
      "Epoch 17: val_loss did not improve from 0.68853\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "300/300 [==============================] - 1239s 4s/step - loss: 0.6900 - accuracy: 0.5238 - val_loss: 0.6898 - val_accuracy: 0.5642 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.5350\n",
      "Epoch 18: val_loss did not improve from 0.68853\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "300/300 [==============================] - 1240s 4s/step - loss: 0.6897 - accuracy: 0.5350 - val_loss: 0.6887 - val_accuracy: 0.5392 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.5213\n",
      "Epoch 19: val_loss improved from 0.68853 to 0.68488, saving model to resnet_model.h5\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "300/300 [==============================] - 1243s 4s/step - loss: 0.6917 - accuracy: 0.5213 - val_loss: 0.6849 - val_accuracy: 0.6000 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.5308\n",
      "Epoch 20: val_loss did not improve from 0.68488\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "300/300 [==============================] - 1239s 4s/step - loss: 0.6888 - accuracy: 0.5308 - val_loss: 0.6856 - val_accuracy: 0.5458 - lr: 2.5000e-04\n",
      "--- Saving Custom Logs for ResNet ---\n",
      "Custom logs for ResNet saved successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Training Parameters\n",
    "EPOCHS = 20  # Number of epochs for training\n",
    "STEPS_PER_EPOCH = 300  # Steps per epoch for training\n",
    "VALIDATION_STEPS = 150  # Steps for validation per epoch\n",
    "\n",
    "# Training ResNet Model\n",
    "print(\"--- Training ResNet Model ---\")\n",
    "history_resnet = resnet_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,  # Number of epochs\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,  # Steps per epoch for training\n",
    "    validation_steps=VALIDATION_STEPS,  # Steps for validation\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_resnet,\n",
    "        csv_logger_resnet,\n",
    "        lr_logger_resnet,\n",
    "        reduce_lr,\n",
    "        metrics_logger_resnet,\n",
    "        epoch_time_logger_resnet,\n",
    "        batch_metrics_logger_resnet\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save Custom Logs\n",
    "print(\"--- Saving Custom Logs for ResNet ---\")\n",
    "log_writer_resnet.close()\n",
    "print(\"Custom logs for ResNet saved successfully.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96020d6-0559-4d1c-b83d-ac4090feef99",
   "metadata": {},
   "source": [
    "### 5.4 Loading Pre-trained Feature Extractors: EfficientNetV2B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cade16-41ee-4790-a742-9d57b33338f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetV2B0 feature extractor loaded successfully.\n",
      "\n",
      "Creating CNN-LSTM model for efficientnetv2-b0...\n",
      "efficientnetv2-b0-based CNN-LSTM model created successfully.\n",
      "\n",
      "--- Compiling EfficientNetV2B0 Model ---\n",
      "EfficientNetV2B0 model compiled successfully.\n",
      "\n",
      "--- EfficientNetV2B0 Model Summary ---\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_2 (TimeDis  (None, 10, 7, 7, 1280)   5919312   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 10, 1280)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               721408    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,657,361\n",
      "Trainable params: 738,049\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load EfficientNetV2B0 feature extractors\n",
    "efficientnet_extractor = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(*TARGET_SIZE, 3))\n",
    "print(\"EfficientNetV2B0 feature extractor loaded successfully.\\n\")\n",
    "\n",
    "# Initialize EfficientNetV2B0 Model\n",
    "efficientnet_model = create_cnn_lstm_model(efficientnet_extractor, SEQUENCE_LENGTH)\n",
    "\n",
    "# Compile EfficientNetV2B0 Model\n",
    "print(\"\\n--- Compiling EfficientNetV2B0 Model ---\")\n",
    "efficientnet_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"EfficientNetV2B0 model compiled successfully.\\n\")\n",
    "\n",
    "# Print EfficientNetV2B0 Model Summary\n",
    "print(\"--- EfficientNetV2B0 Model Summary ---\")\n",
    "efficientnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5888c8-f4a8-49b5-bd4a-74dcf54c07fe",
   "metadata": {},
   "source": [
    "### 5.5 Callbacks for EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69789821-175f-41fd-bf97-55337322450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Up Training Callbacks for EfficientNet ---\n",
      "EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau, and ExcelWriter callbacks initialized for EfficientNetV2B0 model.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks for EfficientNet\n",
    "print(\"--- Setting Up Training Callbacks for EfficientNet ---\")\n",
    "\n",
    "# Standard Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_efficientnet = ModelCheckpoint('efficientnet_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "csv_logger_efficientnet = CSVLogger('efficientnet_training_log.csv')\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',        # Monitor validation loss\n",
    "    factor=0.5,                # Reduce learning rate by half\n",
    "    patience=3,                # Wait for 3 epochs of no improvement\n",
    "    min_lr=1e-6,               # Minimum learning rate\n",
    "    verbose=1                  # Display logs\n",
    ")\n",
    "\n",
    "# Custom Logging Callbacks\n",
    "log_writer_efficientnet = pd.ExcelWriter('efficientnet_training_logs.xlsx', engine='openpyxl')\n",
    "\n",
    "lr_logger_efficientnet = LearningRateLogger(writer=log_writer_efficientnet)\n",
    "metrics_logger_efficientnet = MetricsLogger(validation_data=val_generator, writer=log_writer_efficientnet)\n",
    "epoch_time_logger_efficientnet = EpochTimeLogger(writer=log_writer_efficientnet)\n",
    "batch_metrics_logger_efficientnet = BatchMetricsLogger(filename='efficientnet_batch_logs.csv')\n",
    "\n",
    "print(\"EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau, and ExcelWriter callbacks initialized for EfficientNetV2B0 model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6382c2-e53d-41cf-9213-52ddf91cf832",
   "metadata": {},
   "source": [
    "### 5.6 Training EfficientNet-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38df8d14-4a1b-4391-ab79-a932a3db79cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training EfficientNet Model ---\n",
      "\n",
      "--- Initializing Stratified Sequence Data Generator ---\n",
      "Data Directory: C:\\Users\\atul\\Cropped_Faces\\train\n",
      "Batch Size: 8, Sequence Length: 10, Target Size: (224, 224), Augment: True, Shuffle: True\n",
      "\n",
      "Collecting image paths and labels...\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\train\\real\n",
      "Collected 139606 images for class 'real'.\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\train\\fake\n",
      "Collected 123036 images for class 'fake'.\n",
      "\n",
      "Image collection and shuffling completed for all classes.\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.7026 - accuracy: 0.5075\n",
      "--- Initializing Stratified Sequence Data Generator ---\n",
      "Data Directory: C:\\Users\\atul\\Cropped_Faces\\val\n",
      "Batch Size: 8, Sequence Length: 10, Target Size: (224, 224), Augment: False, Shuffle: True\n",
      "\n",
      "Collecting image paths and labels...\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\val\\real\n",
      "Collected 34902 images for class 'real'.\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\val\\fake\n",
      "Collected 30760 images for class 'fake'.\n",
      "\n",
      "Image collection and shuffling completed for all classes.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69327, saving model to efficientnet_model.h5\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "300/300 [==============================] - 843s 3s/step - loss: 0.7026 - accuracy: 0.5075 - val_loss: 0.6933 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.4875\n",
      "Epoch 2: val_loss improved from 0.69327 to 0.69317, saving model to efficientnet_model.h5\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "300/300 [==============================] - 816s 3s/step - loss: 0.6970 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.4854\n",
      "Epoch 3: val_loss improved from 0.69317 to 0.69315, saving model to efficientnet_model.h5\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "300/300 [==============================] - 810s 3s/step - loss: 0.6945 - accuracy: 0.4854 - val_loss: 0.6932 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.5125\n",
      "Epoch 4: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "300/300 [==============================] - 807s 3s/step - loss: 0.6941 - accuracy: 0.5125 - val_loss: 0.6934 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4975\n",
      "Epoch 5: val_loss did not improve from 0.69315\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "300/300 [==============================] - 798s 3s/step - loss: 0.6934 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5138\n",
      "Epoch 6: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "300/300 [==============================] - 803s 3s/step - loss: 0.6932 - accuracy: 0.5138 - val_loss: 0.6932 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6944 - accuracy: 0.4904\n",
      "Epoch 7: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "300/300 [==============================] - 794s 3s/step - loss: 0.6944 - accuracy: 0.4904 - val_loss: 0.6932 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.4921\n",
      "Epoch 8: val_loss improved from 0.69315 to 0.69315, saving model to efficientnet_model.h5\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "300/300 [==============================] - 792s 3s/step - loss: 0.6938 - accuracy: 0.4921 - val_loss: 0.6932 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5033\n",
      "Epoch 9: val_loss improved from 0.69315 to 0.69315, saving model to efficientnet_model.h5\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "300/300 [==============================] - 784s 3s/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 2.5000e-04\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6934 - accuracy: 0.4900\n",
      "Epoch 10: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "300/300 [==============================] - 786s 3s/step - loss: 0.6934 - accuracy: 0.4900 - val_loss: 0.6932 - val_accuracy: 0.5000 - lr: 2.5000e-04\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 11: val_loss improved from 0.69315 to 0.69315, saving model to efficientnet_model.h5\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "300/300 [==============================] - 783s 3s/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4954\n",
      "Epoch 12: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "300/300 [==============================] - 782s 3s/step - loss: 0.6933 - accuracy: 0.4954 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 1.2500e-04\n",
      "Epoch 13/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5038\n",
      "Epoch 13: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "300/300 [==============================] - 783s 3s/step - loss: 0.6930 - accuracy: 0.5038 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 1.2500e-04\n",
      "Epoch 14/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4892\n",
      "Epoch 14: val_loss did not improve from 0.69315\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "300/300 [==============================] - 775s 3s/step - loss: 0.6933 - accuracy: 0.4892 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 1.2500e-04\n",
      "Epoch 15/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5033\n",
      "Epoch 15: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "300/300 [==============================] - 779s 3s/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 6.2500e-05\n",
      "Epoch 16/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.4950\n",
      "Epoch 16: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "300/300 [==============================] - 777s 3s/step - loss: 0.6933 - accuracy: 0.4950 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 6.2500e-05\n",
      "Epoch 17/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5121\n",
      "Epoch 17: val_loss did not improve from 0.69315\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "300/300 [==============================] - 776s 3s/step - loss: 0.6932 - accuracy: 0.5121 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 6.2500e-05\n",
      "Epoch 18/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5279\n",
      "Epoch 18: val_loss did not improve from 0.69315\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "300/300 [==============================] - 775s 3s/step - loss: 0.6929 - accuracy: 0.5279 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 3.1250e-05\n",
      "Epoch 19/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.4992\n",
      "Epoch 19: val_loss improved from 0.69315 to 0.69314, saving model to efficientnet_model.h5\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "300/300 [==============================] - 775s 3s/step - loss: 0.6931 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 3.1250e-05\n",
      "Epoch 20/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5213\n",
      "Epoch 20: val_loss did not improve from 0.69314\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "300/300 [==============================] - 777s 3s/step - loss: 0.6930 - accuracy: 0.5213 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 3.1250e-05\n",
      "--- Saving Custom Logs for EfficientNet ---\n",
      "Custom logs for EfficientNet saved successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Training Parameters\n",
    "EPOCHS = 20  # Number of epochs for training\n",
    "STEPS_PER_EPOCH = 300  # Steps per epoch for training\n",
    "VALIDATION_STEPS = 150  # Steps for validation per epoch\n",
    "\n",
    "# Training EfficientNet Model\n",
    "print(\"--- Training EfficientNet Model ---\")\n",
    "history_efficientnet = efficientnet_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,  # Number of epochs\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,  # Steps per epoch for training\n",
    "    validation_steps=VALIDATION_STEPS,  # Steps for validation\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_efficientnet,\n",
    "        csv_logger_efficientnet,\n",
    "        lr_logger_efficientnet,\n",
    "        reduce_lr,\n",
    "        metrics_logger_efficientnet,\n",
    "        epoch_time_logger_efficientnet,\n",
    "        batch_metrics_logger_efficientnet\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save Custom Logs\n",
    "print(\"--- Saving Custom Logs for EfficientNet ---\")\n",
    "log_writer_efficientnet.close()\n",
    "print(\"Custom logs for EfficientNet saved successfully.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0113e-af87-4b96-a100-c5868451f559",
   "metadata": {},
   "source": [
    "### 5.7 Loading Pre-trained Feature Extractors: Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b901ab23-073b-4118-bdc4-d428a801e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception feature extractor loaded successfully.\n",
      "\n",
      "Creating CNN-LSTM model for xception...\n",
      "xception-based CNN-LSTM model created successfully.\n",
      "\n",
      "--- Compiling Xception Model ---\n",
      "Xception model compiled successfully.\n",
      "\n",
      "--- Xception Model Summary ---\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_4 (TimeDis  (None, 10, 7, 7, 2048)   20861480  \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 10, 2048)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               1114624   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,992,745\n",
      "Trainable params: 1,131,265\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load Xception feature extractors\n",
    "xception_extractor = Xception(weights='imagenet', include_top=False, input_shape=(*TARGET_SIZE, 3))\n",
    "print(\"Xception feature extractor loaded successfully.\\n\")\n",
    "\n",
    "# Initialize Xception Model\n",
    "xception_model = create_cnn_lstm_model(xception_extractor, SEQUENCE_LENGTH)\n",
    "\n",
    "# Compile Xception Model\n",
    "print(\"\\n--- Compiling Xception Model ---\")\n",
    "xception_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Xception model compiled successfully.\\n\")\n",
    "\n",
    "# Print Xception Model Summary\n",
    "print(\"--- Xception Model Summary ---\")\n",
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdcbf9-2a40-4d67-9163-3af777c9bdf3",
   "metadata": {},
   "source": [
    "### 5.8 Callbacks for Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0196a2fa-2de0-4969-9373-dad367c5c4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Up Training Callbacks for Xception ---\n",
      "EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau, and ExcelWriter callbacks initialized for Xception model.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks for Xception\n",
    "print(\"--- Setting Up Training Callbacks for Xception ---\")\n",
    "\n",
    "# Standard Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_xception = ModelCheckpoint('xception_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "csv_logger_xception = CSVLogger('xception_training_log.csv')\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',        # Monitor validation loss\n",
    "    factor=0.5,                # Reduce learning rate by half\n",
    "    patience=3,                # Wait for 3 epochs of no improvement\n",
    "    min_lr=1e-6,               # Minimum learning rate\n",
    "    verbose=1                  # Display logs\n",
    ")\n",
    "\n",
    "# Custom Logging Callbacks\n",
    "log_writer_xception = pd.ExcelWriter('xception_training_logs.xlsx', engine='openpyxl')\n",
    "\n",
    "lr_logger_xception = LearningRateLogger(writer=log_writer_xception)\n",
    "metrics_logger_xception = MetricsLogger(validation_data=val_generator, writer=log_writer_xception)\n",
    "epoch_time_logger_xception = EpochTimeLogger(writer=log_writer_xception)\n",
    "batch_metrics_logger_xception = BatchMetricsLogger(filename='xception_batch_logs.csv')\n",
    "\n",
    "print(\"EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau, and ExcelWriter callbacks initialized for Xception model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a3d93-e48f-4ba6-bff0-46696abda988",
   "metadata": {},
   "source": [
    "### 5.9 Training Xception-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ade2322-a782-453f-a00b-b59cba3ee56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Xception Model ---\n",
      "\n",
      "--- Initializing Stratified Sequence Data Generator ---\n",
      "Data Directory: C:\\Users\\atul\\Cropped_Faces\\train\n",
      "Batch Size: 5, Sequence Length: 5, Target Size: (224, 224), Augment: True, Shuffle: True\n",
      "\n",
      "Collecting image paths and labels...\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\train\\real\n",
      "Collected 139606 images for class 'real'.\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\train\\fake\n",
      "Collected 123036 images for class 'fake'.\n",
      "\n",
      "Image collection and shuffling completed for all classes.\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.8192\n",
      "--- Initializing Stratified Sequence Data Generator ---\n",
      "Data Directory: C:\\Users\\atul\\Cropped_Faces\\val\n",
      "Batch Size: 5, Sequence Length: 5, Target Size: (224, 224), Augment: False, Shuffle: True\n",
      "\n",
      "Collecting image paths and labels...\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\val\\real\n",
      "Collected 34902 images for class 'real'.\n",
      "Scanning directory: C:\\Users\\atul\\Cropped_Faces\\val\\fake\n",
      "Collected 30760 images for class 'fake'.\n",
      "\n",
      "Image collection and shuffling completed for all classes.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.29075, saving model to xception_model.h5\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "300/300 [==============================] - 391s 1s/step - loss: 0.3845 - accuracy: 0.8192 - val_loss: 0.2908 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9000\n",
      "Epoch 2: val_loss did not improve from 0.29075\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "300/300 [==============================] - 369s 1s/step - loss: 0.2403 - accuracy: 0.9000 - val_loss: 0.3222 - val_accuracy: 0.8617 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9300\n",
      "Epoch 3: val_loss improved from 0.29075 to 0.05856, saving model to xception_model.h5\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "300/300 [==============================] - 369s 1s/step - loss: 0.1671 - accuracy: 0.9300 - val_loss: 0.0586 - val_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9475\n",
      "Epoch 4: val_loss did not improve from 0.05856\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.1375 - accuracy: 0.9475 - val_loss: 0.0587 - val_accuracy: 0.9783 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9458\n",
      "Epoch 5: val_loss did not improve from 0.05856\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.1483 - accuracy: 0.9458 - val_loss: 0.0935 - val_accuracy: 0.9683 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9592\n",
      "Epoch 6: val_loss did not improve from 0.05856\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "300/300 [==============================] - 369s 1s/step - loss: 0.1191 - accuracy: 0.9592 - val_loss: 0.2329 - val_accuracy: 0.9283 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9525\n",
      "Epoch 7: val_loss did not improve from 0.05856\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.1185 - accuracy: 0.9525 - val_loss: 0.0632 - val_accuracy: 0.9767 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9733\n",
      "Epoch 8: val_loss did not improve from 0.05856\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0660 - accuracy: 0.9733 - val_loss: 0.1026 - val_accuracy: 0.9550 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9725\n",
      "Epoch 9: val_loss improved from 0.05856 to 0.02894, saving model to xception_model.h5\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "300/300 [==============================] - 369s 1s/step - loss: 0.0583 - accuracy: 0.9725 - val_loss: 0.0289 - val_accuracy: 0.9933 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9750\n",
      "Epoch 10: val_loss did not improve from 0.02894\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0615 - accuracy: 0.9750 - val_loss: 0.1136 - val_accuracy: 0.9633 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9758\n",
      "Epoch 11: val_loss improved from 0.02894 to 0.02581, saving model to xception_model.h5\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "300/300 [==============================] - 370s 1s/step - loss: 0.0584 - accuracy: 0.9758 - val_loss: 0.0258 - val_accuracy: 0.9867 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9758\n",
      "Epoch 12: val_loss did not improve from 0.02581\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0642 - accuracy: 0.9758 - val_loss: 0.0506 - val_accuracy: 0.9817 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9867\n",
      "Epoch 13: val_loss did not improve from 0.02581\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0424 - accuracy: 0.9867 - val_loss: 0.0396 - val_accuracy: 0.9867 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9800\n",
      "Epoch 14: val_loss did not improve from 0.02581\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0553 - accuracy: 0.9800 - val_loss: 0.0688 - val_accuracy: 0.9733 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9858\n",
      "Epoch 15: val_loss did not improve from 0.02581\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0356 - accuracy: 0.9858 - val_loss: 0.0268 - val_accuracy: 0.9933 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9817\n",
      "Epoch 16: val_loss did not improve from 0.02581\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0462 - accuracy: 0.9817 - val_loss: 0.0602 - val_accuracy: 0.9733 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9883\n",
      "Epoch 17: val_loss did not improve from 0.02581\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0427 - accuracy: 0.9883 - val_loss: 0.0375 - val_accuracy: 0.9883 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9833\n",
      "Epoch 18: val_loss did not improve from 0.02581\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "300/300 [==============================] - 367s 1s/step - loss: 0.0460 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9883 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9867\n",
      "Epoch 19: val_loss did not improve from 0.02581\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0393 - accuracy: 0.9867 - val_loss: 0.0384 - val_accuracy: 0.9850 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9925\n",
      "Epoch 20: val_loss did not improve from 0.02581\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "300/300 [==============================] - 368s 1s/step - loss: 0.0279 - accuracy: 0.9925 - val_loss: 0.0290 - val_accuracy: 0.9833 - lr: 1.2500e-04\n",
      "--- Saving Custom Logs for Xception ---\n",
      "Custom logs for Xception saved successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Training Parameters\n",
    "EPOCHS = 20  # Number of epochs for training\n",
    "STEPS_PER_EPOCH = 300  # Steps per epoch for training\n",
    "VALIDATION_STEPS = 150  # Steps for validation per epoch\n",
    "\n",
    "# Training Xception Model\n",
    "print(\"--- Training Xception Model ---\")\n",
    "history_xception = xception_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=EPOCHS,  # Number of epochs\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,  # Steps per epoch for training\n",
    "    validation_steps=VALIDATION_STEPS,  # Steps for validation\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        checkpoint_xception,\n",
    "        csv_logger_xception,\n",
    "        lr_logger_xception,\n",
    "        reduce_lr,\n",
    "        metrics_logger_xception,\n",
    "        epoch_time_logger_xception,\n",
    "        batch_metrics_logger_xception\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save Custom Logs\n",
    "print(\"--- Saving Custom Logs for Xception ---\")\n",
    "log_writer_xception.close()\n",
    "print(\"Custom logs for Xception saved successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690466e0-1973-420f-947a-121795053c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Cleared.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"Session Cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcecda8-495d-42df-a660-ea179886cab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17a833-8070-46de-a488-9640cde05b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
